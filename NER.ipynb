{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-22 01:07:48.883330: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-22 01:07:48.908196: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-22 01:07:49.383532: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/pokropow/programming/Named Entity Recognition/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from ast import literal_eval\n",
    "import functools\n",
    "from typing import Any\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import lightning as pl\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, BertForTokenClassification\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../ner.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "all_tags = sorted(functools.reduce(lambda x, y: x.union(y), df['Tag'].map(literal_eval).map(set).to_list()))\n",
    "\n",
    "def align_labels(tags:list[str], tokenized_inputs: transformers.BatchEncoding, batch_index:int):\n",
    "    word_ids = tokenized_inputs.word_ids(batch_index=batch_index)\n",
    "    label_ids = []\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            label_ids.append(-100)\n",
    "        else:\n",
    "            tag = tags[word_idx]\n",
    "            label_ids.append(all_tags.index(tag))\n",
    "    return label_ids\n",
    "\n",
    "def tokenize(df:pd.DataFrame):\n",
    "    sentences = df['Sentence'].map(lambda x: x.split(\" \")).to_list()\n",
    "    tags = df['Tag'].map(literal_eval).to_list()\n",
    "    tokenized_inputs = tokenizer(\n",
    "        sentences, truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    aligned_labels = [\n",
    "        align_labels(sentence_tags, tokenized_inputs, i)\n",
    "        for i, sentence_tags in enumerate(tags)\n",
    "    ]\n",
    "    \n",
    "    tokenized_inputs[\"labels\"] = aligned_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized = tokenize(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERDataset(Dataset):\n",
    "    def __init__(self, tokenized_data: transformers.BatchEncoding, indexes: list[int]) -> None:\n",
    "        super().__init__()\n",
    "        self.tokenized_data = tokenized_data\n",
    "        self.indexes = indexes\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        ids = self.tokenized_data[index].ids\n",
    "        attention_mask = self.tokenized_data[index].attention_mask\n",
    "        targets = self.tokenized_data[\"labels\"][index]\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(ids, dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(targets, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indexes)\n",
    "\n",
    "\n",
    "def collate(samples):\n",
    "    padding_id = tokenizer.get_vocab()[\"[PAD]\"]\n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "        [sample[\"input_ids\"] for sample in samples], batch_first=True, padding_value=padding_id\n",
    "    )\n",
    "    attention_mask = torch.nn.utils.rnn.pad_sequence(\n",
    "        [sample[\"attention_mask\"] for sample in samples], batch_first=True, padding_value=0\n",
    "    )\n",
    "    labels = torch.nn.utils.rnn.pad_sequence(\n",
    "        [sample[\"labels\"] for sample in samples], batch_first=True, padding_value=-100\n",
    "    )\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "indexes = list(range(len(df)))\n",
    "random.Random(1337).shuffle(indexes)\n",
    "\n",
    "train_indexes = indexes[:38367]\n",
    "val_indexes = indexes[38367:]\n",
    "\n",
    "# train_indexes = indexes[:100]\n",
    "# val_indexes = indexes[-100:]\n",
    "\n",
    "train_dataset = NERDataset(tokenized, train_indexes)\n",
    "val_dataset = NERDataset(tokenized, val_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricsCalculator:\n",
    "    def __init__(self) -> None:\n",
    "        self.targets = []\n",
    "        self.predictions = []\n",
    "\n",
    "    def update(self, logits: torch.Tensor, targets: torch.Tensor) -> None:\n",
    "        prediction = logits.argmax(-1).view(-1)\n",
    "        targets = targets.view(-1)\n",
    "\n",
    "        prediction = prediction[targets != -100].cpu().numpy()\n",
    "        targets = targets[targets != -100].cpu().numpy()\n",
    "\n",
    "        self.predictions.append(prediction)\n",
    "        self.targets.append(targets)\n",
    "\n",
    "    def get_confusion_matrix(self) -> np.ndarray:\n",
    "        y_true = np.concatenate(self.targets)\n",
    "        y_pred = np.concatenate(self.predictions)\n",
    "\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot()\n",
    "        ConfusionMatrixDisplay.from_predictions(\n",
    "            y_true,\n",
    "            y_pred,\n",
    "            display_labels=all_tags,\n",
    "            labels=list(range(len(all_tags))),\n",
    "            ax=ax,\n",
    "            xticks_rotation=\"vertical\",\n",
    "            normalize=\"true\",\n",
    "            values_format=\".2f\",\n",
    "            text_kw={\"fontsize\": \"xx-small\"}\n",
    "        )\n",
    "        fig.canvas.draw()\n",
    "        data = np.array(fig.canvas.renderer.buffer_rgba())\n",
    "        plt.close(fig)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def get_metrics(self) -> dict[str, float]:\n",
    "        y_true = np.concatenate(self.targets)\n",
    "        y_pred = np.concatenate(self.predictions)\n",
    "\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "\n",
    "        return {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"f1\": f1,\n",
    "        }\n",
    "\n",
    "    def clear(self):\n",
    "        self.targets.clear()\n",
    "        self.predictions.clear()\n",
    "\n",
    "\n",
    "class NERModule(pl.LightningModule):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.model = BertForTokenClassification.from_pretrained(\n",
    "            \"bert-base-uncased\", torch_dtype=torch.float32, attn_implementation=\"sdpa\", num_labels=len(all_tags)\n",
    "        )\n",
    "        self.train_metrics_calculator = MetricsCalculator()\n",
    "        self.val_metrics_calculator = MetricsCalculator()\n",
    "\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(params=self.model.parameters(), lr=1e-05)\n",
    "        return optimizer\n",
    "\n",
    "    def train_dataloader(self) -> Any:\n",
    "        return DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0, collate_fn=collate)\n",
    "\n",
    "    def val_dataloader(self) -> Any:\n",
    "        return DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0, collate_fn=collate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(**x)\n",
    "\n",
    "    def training_step(self, x):\n",
    "        output = self(x)\n",
    "        loss = output.loss\n",
    "        logits = output.logits\n",
    "\n",
    "        self.train_metrics_calculator.update(logits, x[\"labels\"])\n",
    "        self.losses.append(loss.item())\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, x):\n",
    "        output = self(x)\n",
    "        loss = output.loss\n",
    "        logits = output.logits\n",
    "\n",
    "        self.val_metrics_calculator.update(logits, x[\"labels\"])\n",
    "        self.val_losses.append(loss.item())\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self) -> None:\n",
    "        metrics = self.train_metrics_calculator.get_metrics()\n",
    "        metrics[\"loss\"] = np.mean(self.losses)\n",
    "        self.log_dict(metrics, on_epoch=True)\n",
    "        self.losses.clear()\n",
    "        self.train_metrics_calculator.clear()\n",
    "\n",
    "    def on_validation_epoch_end(self) -> None:\n",
    "        metrics = self.val_metrics_calculator.get_metrics()\n",
    "        metrics[\"loss\"] = np.mean(self.val_losses)\n",
    "        metrics = {\"val_\" + k: v for k, v in metrics.items()}\n",
    "\n",
    "        confusion_mat = self.val_metrics_calculator.get_confusion_matrix()\n",
    "\n",
    "        self.log_dict(metrics, on_epoch=True)\n",
    "        self.val_losses.clear()\n",
    "        self.val_metrics_calculator.clear()\n",
    "        self.logger.experiment.add_image(\n",
    "            \"confusion matrix\", confusion_mat.astype(np.float32) / 255, dataformats=\"HWC\", global_step=self.global_step\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/pokropow/programming/Named Entity Recognition/venv/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:652: Checkpoint directory /home/pokropow/programming/Named Entity Recognition/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                       | Params | Mode\n",
      "------------------------------------------------------------\n",
      "0 | model | BertForTokenClassification | 108 M  | eval\n",
      "------------------------------------------------------------\n",
      "108 M     Trainable params\n",
      "0         Non-trainable params\n",
      "108 M     Total params\n",
      "435.619   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pokropow/programming/Named Entity Recognition/venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pokropow/programming/Named Entity Recognition/venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 1199/1199 [00:47<00:00, 25.27it/s, v_num=8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 1199/1199 [00:47<00:00, 25.11it/s, v_num=8]\n"
     ]
    }
   ],
   "source": [
    "model = NERModule()\n",
    "logger = TensorBoardLogger(\"tb_logs\")\n",
    "checkpoint_callback = ModelCheckpoint(dirpath=\"checkpoints/\", save_top_k=2, monitor=\"val_f1\", mode=\"max\")\n",
    "trainer = pl.Trainer(max_epochs=20, precision=\"16-mixed\", accelerator='gpu', logger=logger, callbacks=[checkpoint_callback])\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
